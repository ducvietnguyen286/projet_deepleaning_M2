{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"},"colab":{"name":"reseaux_neurones.ipynb","provenance":[],"collapsed_sections":["49c6fd22","70620a70","4c1e60b5","287bfc31"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"3824fdbc"},"source":["# Réseaux de neurones"],"id":"3824fdbc"},{"cell_type":"markdown","metadata":{"id":"ee0bddcf"},"source":["Voici la liste des réseaux de neurones que nous allons implémenter :\n","\n","1. DNN\n","2. LSTM\n","3. GRU\n","4. BRNN\n","5. RCNN"],"id":"ee0bddcf"},{"cell_type":"markdown","metadata":{"id":"be7cd05a"},"source":["### Initialisation"],"id":"be7cd05a"},{"cell_type":"code","metadata":{"id":"e317d7ac","executionInfo":{"status":"ok","timestamp":1638814625442,"user_tz":-60,"elapsed":3705,"user":{"displayName":"Théo Gayant","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14159994340801932176"}}},"source":["import keras\n","from keras import models\n","from keras.models import Sequential, Model\n","from keras.layers import Dense, Dropout, LSTM, Bidirectional, Input, Reshape, GRU, Convolution1D, Flatten\n","from sklearn import metrics\n","from keras import backend as K\n","from sklearn.model_selection import train_test_split\n","import time"],"id":"e317d7ac","execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"9f53771e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638814630767,"user_tz":-60,"elapsed":5360,"user":{"displayName":"Théo Gayant","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14159994340801932176"}},"outputId":"bd99977e-3adb-4041-faf1-86d75f62f98f"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","import pandas as pd\n","import numpy as np\n","from numpy import mean\n","from numpy import std\n","from numpy import dstack\n","from pandas import read_csv\n","from matplotlib import pyplot\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import Dropout\n","from keras.layers.convolutional import Conv1D\n","from keras.layers.convolutional import MaxPooling1D, AveragePooling1D\n","from keras.layers import GlobalMaxPooling1D\n","from keras.layers import GlobalAveragePooling1D\n","import pathlib\n","import joblib\n","from sklearn import svm\n","import sklearn.model_selection as model_selection\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import recall_score\n","import time\n","import os\n","!pip install memory_profiler\n","import tracemalloc"],"id":"9f53771e","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting memory_profiler\n","  Downloading memory_profiler-0.58.0.tar.gz (36 kB)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory_profiler) (5.4.8)\n","Building wheels for collected packages: memory-profiler\n","  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for memory-profiler: filename=memory_profiler-0.58.0-py3-none-any.whl size=30190 sha256=2b682f762ffb95693beea6d38278e67822f4e94b7a99340ca2e4336b9274dc30\n","  Stored in directory: /root/.cache/pip/wheels/56/19/d5/8cad06661aec65a04a0d6785b1a5ad035cb645b1772a4a0882\n","Successfully built memory-profiler\n","Installing collected packages: memory-profiler\n","Successfully installed memory-profiler-0.58.0\n"]}]},{"cell_type":"code","metadata":{"id":"a923d8cc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638814655333,"user_tz":-60,"elapsed":3332,"user":{"displayName":"Théo Gayant","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14159994340801932176"}},"outputId":"67433942-539b-437a-da90-d401bc3f1f7e"},"source":["pip install pyyaml h5py"],"id":"a923d8cc","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (3.13)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.19.5)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n"]}]},{"cell_type":"code","metadata":{"id":"d42911fc","executionInfo":{"status":"ok","timestamp":1638814657625,"user_tz":-60,"elapsed":307,"user":{"displayName":"Théo Gayant","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14159994340801932176"}}},"source":["%load_ext memory_profiler"],"id":"d42911fc","execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"a4df5657","executionInfo":{"status":"ok","timestamp":1638814762108,"user_tz":-60,"elapsed":566,"user":{"displayName":"Théo Gayant","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14159994340801932176"}}},"source":["df = pd.read_csv(\"opinion_fact_news_pretraiter.csv\")\n","df = df.drop(columns=[\"Unnamed: 0\"])"],"id":"a4df5657","execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"15f474a3","executionInfo":{"status":"ok","timestamp":1638814767433,"user_tz":-60,"elapsed":242,"user":{"displayName":"Théo Gayant","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14159994340801932176"}}},"source":["df.loc[df[\"label\"] == \"fact\",\"label\"] = 1\n","df.loc[df[\"label\"] == \"opinion\",\"label\"] = 0\n","df['label']=df['label'].astype('int')"],"id":"15f474a3","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"d55ca883","executionInfo":{"status":"ok","timestamp":1638814768805,"user_tz":-60,"elapsed":13,"user":{"displayName":"Théo Gayant","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14159994340801932176"}}},"source":["X = df['body'] \n","ylabels = df['label'] "],"id":"d55ca883","execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"41c4f537"},"source":["### Bow (Bag of Word)"],"id":"41c4f537"},{"cell_type":"code","metadata":{"id":"2a25ef05","executionInfo":{"status":"ok","timestamp":1638814791728,"user_tz":-60,"elapsed":7920,"user":{"displayName":"Théo Gayant","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14159994340801932176"}}},"source":["import gensim\n","from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n","X_clear = []\n","for x in X:\n"," paragraph = gensim.utils.simple_preprocess(x)\n"," paragraph = ' '.join(paragraph)\n"," X_clear.append(paragraph)\n","\n","\n","vectorizer = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n","vectorizer.fit(X_clear)\n","\n","\n","X_data_count = vectorizer.transform(X_clear)"],"id":"2a25ef05","execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a7687dde"},"source":["### Tf-Idf  (Term Frequency-Inverse Document Frequency)"],"id":"a7687dde"},{"cell_type":"code","metadata":{"id":"a750b596","executionInfo":{"status":"ok","timestamp":1638814836436,"user_tz":-60,"elapsed":44736,"user":{"displayName":"Théo Gayant","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14159994340801932176"}}},"source":["tfidf_vect = TfidfVectorizer(analyzer='word', max_features=30000)\n","tfidf_vect.fit(X_clear)\n","X_data_tfidf =  tfidf_vect.transform(X_clear)\n","\n","tfidf_vect_ngram = TfidfVectorizer(analyzer='word', max_features=30000, ngram_range=(3, 3))\n","tfidf_vect_ngram.fit(X_clear)\n","X_data_tfidf_ngram =  tfidf_vect_ngram.transform(X_clear)\n","\n","tfidf_vect_ngram_char = TfidfVectorizer(analyzer='char', max_features=30000, ngram_range=(3, 3))\n","tfidf_vect_ngram_char.fit(X_clear)\n","X_data_tfidf_ngram_char =  tfidf_vect_ngram_char.transform(X_clear)"],"id":"a750b596","execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nD73WSiI9Vv9"},"source":["Après avoir implémenté TF-IDF, je remarque que la matrice que nous obtenons a une taille très grande, ainsi pour traiter cette matrice de manière brute nécessiteraient trop de temps et de mémoire.\n","\n","On utilisera l'algorithme SVD (décomposition en valeurs singulières) pour réduire la dimension des données de la matrice que nous avons obtenue, tout en préservant les propriétés de la matrice d'origine."],"id":"nD73WSiI9Vv9"},{"cell_type":"code","metadata":{"id":"9d48dac4","executionInfo":{"status":"ok","timestamp":1638814892081,"user_tz":-60,"elapsed":54544,"user":{"displayName":"Théo Gayant","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14159994340801932176"}}},"source":["from sklearn.decomposition import TruncatedSVD\n","\n","svd = TruncatedSVD(n_components=300, random_state=42)\n","svd.fit(X_data_tfidf)\n","X_data_tfidf_svd = svd.transform(X_data_tfidf)\n","\n","svd_ngram = TruncatedSVD(n_components=300, random_state=42)\n","svd_ngram.fit(X_data_tfidf_ngram)\n","X_data_tfidf_ngram_svd = svd_ngram.transform(X_data_tfidf_ngram)\n","\n","svd_ngram_char = TruncatedSVD(n_components=300, random_state=42)\n","svd_ngram_char.fit(X_data_tfidf_ngram_char)\n","X_data_tfidf_ngram_char_svd = svd_ngram_char.transform(X_data_tfidf_ngram_char)"],"id":"9d48dac4","execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"66f58b2f"},"source":["## DNN"],"id":"66f58b2f"},{"cell_type":"code","metadata":{"id":"44bJ68oP_StN"},"source":["def evaluate_model_DNN(nom, X_data,Y_data,n_epochs, drop_Out):\n","  verbose , batch_size = 0 , 32\n","  X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size=0.2, random_state=42,stratify=Y_data)\n","  \n","  input_layer = Input(shape=(300,))\n","\n","  layer = Dense(1024, activation='relu')(input_layer)\n","  layer = Dropout(drop_Out)(layer)\n","  layer = Dense(1024, activation='relu')(layer)\n","  layer = Dense(512, activation='relu')(layer)\n","  output_layer = Dense(1, activation='sigmoid')(layer)\n","  \n","  model = models.Model(input_layer, output_layer)\n","\n","  start_time_train = time.time()\n","  model.compile(\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n","  model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=n_epochs, batch_size=512)\n","  end_time_train = time.time()\n","  time_train= (end_time_train - start_time_train)*1000\n","  time_train_val = round(time_train)\n","\n","\n","  accuracy = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=0)\n","  start_time_test = time.time()\n","  y_pred = model.predict(X_test)\n","  end_time_test = time.time()\n","  time_test= (end_time_test - start_time_test)*1000\n","  time_test_val=round(time_test)\n","  y_pred = np.around(y_pred, 0)\n","  #df_resultDL.loc[(len(df_resultDL)+1)] = {'Nom': nom, 'Drop_out': drop_Out,'Accuracy': metrics.accuracy_score(test_predictions, Y_test), 'Precision': metrics.precision_score(test_predictions, Y_test), 'Recall' : metrics.recall_score(test_predictions, Y_test)}\n","  print(\"Accuracy: \", metrics.accuracy_score(y_pred, Y_test))\n","  #co the sai---------------------------------------------\n","  start_time_test = time.time()\n","  model.predict(X_test)\n","  end_time_test = time.time()\n","  time_test= (end_time_test - start_time_test)*1000\n","  time_test_val=round(time_test)\n","  #--------------------------------------------------------------------------\n","  tracemalloc.start()\n","  model.predict(X_test)\n","  snapshot = tracemalloc.take_snapshot()\n","  top_stats = snapshot.statistics('traceback')\n","  stat = top_stats[0]\n","  mem_test = round(stat.size/1024)\n","  #------------------------------------------------------------\n","  # converture y_pred\n"," # output_TF=model.predict(X_test)\n"," #y_pred=(np.argmax(output_TF,axis=1)+1)\n","  \n","  precision = metrics.precision_score(y_pred,Y_test)\n","  precision_val=round(precision,4)\n","\n","  recall= metrics.recall_score(y_pred,Y_test,average='macro')\n","  recal_val=round(recall,4)\n"," \n","  f1=metrics.f1_score(y_pred,Y_test,average='macro')\n","  f1_val=round(f1,4)\n","  \n","  accuracy_test = metrics.accuracy_score(y_pred,Y_test)\n","  accuracy_test_val = round(accuracy_test,4)\n","\n","  params = np.sum([np.prod(v.get_shape()) for v in model.trainable_weights])\n","  \n","  export_dir=\"save_model/DNN\"\n","\n","  model.save(export_dir)\n","\n","  model=tf.keras.models.load_model(export_dir)  \n","\n","  taille= os.stat('save_model/DNN').st_size\n","\n","  return [nom,n_epochs,drop_Out,precision_val,recal_val,f1_val,accuracy_test_val,time_train_val, time_test_val,mem_test,params,taille]"],"id":"44bJ68oP_StN","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pdmjVl-Y_V7z"},"source":["scores = []\n","def run_experiment():\n","  with open('resultat_DNN.txt', 'w') as f:\n","    for dropOut in [0, 0.2,0.5,0.8]:\n","      score= evaluate_model_DNN(nom='Deep_Neural_Network', X_data = X_data_tfidf_svd, Y_data = ylabels, n_epochs=20,drop_Out= dropOut)\n","      #print(score)\n","      f.write(\"{0}\".format(score))\n","      scores.append(score)\n","    return scores"],"id":"pdmjVl-Y_V7z","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"995d4b2a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638815319196,"user_tz":-60,"elapsed":87252,"user":{"displayName":"Théo Gayant","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14159994340801932176"}},"outputId":"f24c5c84-d66d-4c47-9332-eb7487bd64df"},"source":["scores=run_experiment()"],"id":"995d4b2a","execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","16/16 [==============================] - 3s 144ms/step - loss: 0.4431 - accuracy: 0.7916 - val_loss: 0.0609 - val_accuracy: 0.9840\n","Epoch 2/20\n","16/16 [==============================] - 2s 129ms/step - loss: 0.0818 - accuracy: 0.9711 - val_loss: 0.0353 - val_accuracy: 0.9880\n","Epoch 3/20\n","16/16 [==============================] - 2s 130ms/step - loss: 0.0456 - accuracy: 0.9843 - val_loss: 0.0277 - val_accuracy: 0.9910\n","Epoch 4/20\n","16/16 [==============================] - 2s 131ms/step - loss: 0.0350 - accuracy: 0.9877 - val_loss: 0.0255 - val_accuracy: 0.9925\n","Epoch 5/20\n","16/16 [==============================] - 2s 128ms/step - loss: 0.0284 - accuracy: 0.9894 - val_loss: 0.0228 - val_accuracy: 0.9935\n","Epoch 6/20\n","16/16 [==============================] - 2s 131ms/step - loss: 0.0236 - accuracy: 0.9919 - val_loss: 0.0235 - val_accuracy: 0.9920\n","Epoch 7/20\n","16/16 [==============================] - 2s 132ms/step - loss: 0.0181 - accuracy: 0.9934 - val_loss: 0.0254 - val_accuracy: 0.9910\n","Epoch 8/20\n","16/16 [==============================] - 2s 129ms/step - loss: 0.0180 - accuracy: 0.9935 - val_loss: 0.0273 - val_accuracy: 0.9910\n","Epoch 9/20\n","16/16 [==============================] - 2s 130ms/step - loss: 0.0153 - accuracy: 0.9945 - val_loss: 0.0261 - val_accuracy: 0.9900\n","Epoch 10/20\n","16/16 [==============================] - 2s 130ms/step - loss: 0.0126 - accuracy: 0.9955 - val_loss: 0.0258 - val_accuracy: 0.9900\n","Epoch 11/20\n","16/16 [==============================] - 2s 132ms/step - loss: 0.0124 - accuracy: 0.9965 - val_loss: 0.0247 - val_accuracy: 0.9920\n","Epoch 12/20\n","16/16 [==============================] - 2s 130ms/step - loss: 0.0126 - accuracy: 0.9954 - val_loss: 0.0234 - val_accuracy: 0.9930\n","Epoch 13/20\n","16/16 [==============================] - 2s 133ms/step - loss: 0.0082 - accuracy: 0.9969 - val_loss: 0.0249 - val_accuracy: 0.9925\n","Epoch 14/20\n","16/16 [==============================] - 2s 132ms/step - loss: 0.0121 - accuracy: 0.9958 - val_loss: 0.0245 - val_accuracy: 0.9925\n","Epoch 15/20\n","16/16 [==============================] - 2s 131ms/step - loss: 0.0127 - accuracy: 0.9961 - val_loss: 0.0254 - val_accuracy: 0.9930\n","Epoch 16/20\n","16/16 [==============================] - 2s 131ms/step - loss: 0.0069 - accuracy: 0.9983 - val_loss: 0.0306 - val_accuracy: 0.9915\n","Epoch 17/20\n","16/16 [==============================] - 2s 130ms/step - loss: 0.0072 - accuracy: 0.9971 - val_loss: 0.0306 - val_accuracy: 0.9920\n","Epoch 18/20\n","16/16 [==============================] - 2s 129ms/step - loss: 0.0063 - accuracy: 0.9981 - val_loss: 0.0289 - val_accuracy: 0.9925\n","Epoch 19/20\n","16/16 [==============================] - 2s 132ms/step - loss: 0.0065 - accuracy: 0.9976 - val_loss: 0.0298 - val_accuracy: 0.9930\n","Epoch 20/20\n","16/16 [==============================] - 2s 130ms/step - loss: 0.0083 - accuracy: 0.9977 - val_loss: 0.0309 - val_accuracy: 0.9915\n","Accuracy:  0.9915\n","INFO:tensorflow:Assets written to: DNN/assets\n"]}]},{"cell_type":"markdown","metadata":{"id":"49c6fd22"},"source":["## LSTM"],"id":"49c6fd22"},{"cell_type":"code","metadata":{"id":"8c775362"},"source":["def evaluate_model_LSTM(nom, X_data,Y_data,n_epochs, drop_Out):\n","  verbose , batch_size = 0 , 32\n","  X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size=0.2, random_state=42,stratify=Y_data)\n","  \n","  input_layer = Input(shape=(300,))\n","\n","  layer = Reshape((10, 30))(input_layer)\n","  layer = LSTM(256, activation='relu', return_sequences=True)(layer)\n","  layer = Dropout(drop_Out)(layer)\n","  layer = LSTM(128, activation='relu')(layer)\n","  layer = Dense(512, activation='relu')(layer)\n","  layer = Dense(512, activation='relu')(layer)\n","  layer = Dense(128, activation='relu')(layer)\n","  output_layer = Dense(1, activation='sigmoid')(layer)\n","\n","  model = models.Model(input_layer, output_layer)\n","\n","  start_time_train = time.time()\n","  model.compile(\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n","  model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=n_epochs, batch_size=512)\n","  end_time_train = time.time()\n","  time_train= (end_time_train - start_time_train)*1000\n","  time_train_val = round(time_train)\n","\n","\n","  accuracy = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=0)\n","  start_time_test = time.time()\n","  y_pred = model.predict(X_test)\n","  end_time_test = time.time()\n","  time_test= (end_time_test - start_time_test)*1000\n","  time_test_val=round(time_test)\n","  y_pred = np.around(y_pred, 0)\n","  #df_resultDL.loc[(len(df_resultDL)+1)] = {'Nom': nom, 'Drop_out': drop_Out,'Accuracy': metrics.accuracy_score(test_predictions, Y_test), 'Precision': metrics.precision_score(test_predictions, Y_test), 'Recall' : metrics.recall_score(test_predictions, Y_test)}\n","  print(\"Accuracy: \", metrics.accuracy_score(y_pred, Y_test))\n","  #co the sai---------------------------------------------\n","  start_time_test = time.time()\n","  model.predict(X_test)\n","  end_time_test = time.time()\n","  time_test= (end_time_test - start_time_test)*1000\n","  time_test_val=round(time_test)\n","  #--------------------------------------------------------------------------\n","  tracemalloc.start()\n","  model.predict(X_test)\n","  snapshot = tracemalloc.take_snapshot()\n","  top_stats = snapshot.statistics('traceback')\n","  stat = top_stats[0]\n","  mem_test = round(stat.size/1024)\n","  #------------------------------------------------------------\n","  # converture y_pred\n"," # output_TF=model.predict(X_test)\n"," #y_pred=(np.argmax(output_TF,axis=1)+1)\n","\n","  precision = metrics.precision_score(y_pred,Y_test)\n","  precision_val=round(precision,4)\n"," \n","  recall= metrics.recall_score(y_pred,Y_test,average='macro')\n","  recal_val=round(recall,4)\n","\n","  f1=metrics.f1_score(y_pred,Y_test,average='macro')\n","  f1_val=round(f1,4)\n"," \n","  accuracy_test = metrics.accuracy_score(y_pred,Y_test)\n","  accuracy_test_val = round(accuracy_test,4)\n","\n","  params = np.sum([np.prod(v.get_shape()) for v in model.trainable_weights])\n","\n","  export_dir=\"save_model/DNN\"\n","\n","  model.save(export_dir)\n","\n","  model=tf.keras.models.load_model(export_dir)  \n","\n","  taille= os.stat('save_model/DNN').st_size   \n","\n","  return [nom,n_epochs,drop_Out,precision_val,recal_val,f1_val,accuracy_test_val,time_train_val, time_test_val,mem_test,params,taille]"],"id":"8c775362","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7f97fb76"},"source":["scores = []\n","def run_experiment():\n","  with open('save_model/resultat_LSTM.txt', 'w') as f:\n","    for dropOut in [0, 0.2,0.5,0.8]:\n","      score= evaluate_model_LSTM(nom='LSTM', X_data = X_data_tfidf_svd, Y_data = ylabels, n_epochs=20,drop_Out= dropOut)\n","      #print(score)\n","      f.write(\"{0}\".format(score))\n","      scores.append(score)\n","    return scores"],"id":"7f97fb76","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d02b107d"},"source":["scores=run_experiment()"],"id":"d02b107d","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"70620a70"},"source":["## GRU"],"id":"70620a70"},{"cell_type":"code","metadata":{"id":"bc2b32dd"},"source":["def evaluate_model_GRU(nom, X_data,Y_data,n_epochs, drop_Out):\n","  verbose , batch_size = 0 , 32\n","  X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size=0.2, random_state=42,stratify=Y_data)\n","  \n","  input_layer = Input(shape=(300,))\n","\n","  layer = Reshape((10, 30))(input_layer)\n","  layer = GRU(128, activation='relu')(layer)\n","  layer = Dropout(drop_Out)(layer)\n"," # layer = GRU(64, activation='relu')(layer)\n","  layer = Dense(256, activation='relu')(layer)\n","  layer = Dense(128, activation='relu')(layer)\n","  output_layer = Dense(1, activation='sigmoid')(layer)\n","\n","  model = models.Model(input_layer, output_layer)\n","\n","  start_time_train = time.time()\n","  model.compile(\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n","  model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=n_epochs, batch_size=512)\n","  end_time_train = time.time()\n","  time_train= (end_time_train - start_time_train)*1000\n","  time_train_val = round(time_train)\n","\n","\n","  accuracy = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=0)\n","  start_time_test = time.time()\n","  y_pred = model.predict(X_test)\n","  end_time_test = time.time()\n","  time_test= (end_time_test - start_time_test)*1000\n","  time_test_val=round(time_test)\n","  y_pred = np.around(y_pred, 0)\n","\n","  print(\"Accuracy: \", metrics.accuracy_score(y_pred, Y_test))\n","\n","  start_time_test = time.time()\n","  model.predict(X_test)\n","  end_time_test = time.time()\n","  time_test= (end_time_test - start_time_test)*1000\n","  time_test_val=round(time_test)\n","\n","  tracemalloc.start()\n","  model.predict(X_test)\n","  snapshot = tracemalloc.take_snapshot()\n","  top_stats = snapshot.statistics('traceback')\n","  stat = top_stats[0]\n","  mem_test = round(stat.size/1024)\n","  \n","  precision = metrics.precision_score(y_pred,Y_test)\n","  precision_val=round(precision,4)\n","\n","  recall= metrics.recall_score(y_pred,Y_test,average='macro')\n","  recal_val=round(recall,4)\n","\n","  f1=metrics.f1_score(y_pred,Y_test,average='macro')\n","  f1_val=round(f1,4)\n","\n","  accuracy_test = metrics.accuracy_score(y_pred,Y_test)\n","  accuracy_test_val = round(accuracy_test,4)\n","\n","  params = np.sum([np.prod(v.get_shape()) for v in model.trainable_weights])\n","\n","  export_dir=\"save_model/GRU\"\n","\n","  model.save(export_dir)\n","\n","  model=tf.keras.models.load_model(export_dir)  \n","\n","  taille= os.stat('save_model/GRU').st_size   \n","\n","  return [nom,n_epochs,drop_Out,precision_val,recal_val,f1_val,accuracy_test_val,time_train_val, time_test_val,mem_test,params,taille]"],"id":"bc2b32dd","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"372f6a01"},"source":["scores = []\n","def run_experiment():\n","  with open('save_model/resultat_GRU.txt', 'w') as f:\n","    for dropOut in [0, 0.2,0.5,0.8]:\n","      score= evaluate_model_GRU(nom='GRU', X_data = X_data_tfidf_svd, Y_data = ylabels, n_epochs=20,drop_Out= dropOut)\n","      #print(score)\n","      f.write(\"{0}\".format(score))\n","      scores.append(score)\n","    return scores"],"id":"372f6a01","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cfa8bf5b"},"source":["scores=run_experiment()"],"id":"cfa8bf5b","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4c1e60b5"},"source":["## Bidirectional RNN"],"id":"4c1e60b5"},{"cell_type":"code","metadata":{"id":"68c3121b"},"source":["def evaluate_model_BRNN(nom, X_data,Y_data,n_epochs, drop_Out):\n","  verbose , batch_size = 0 , 32\n","  X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size=0.2, random_state=42,stratify=Y_data)\n","  \n","  input_layer = Input(shape=(300,))\n","\n","  layer = Reshape((10, 30))(input_layer)\n","  layer = Bidirectional(GRU(128, activation='relu'))(layer)\n","  layer = Dropout(drop_Out)(layer)\n","  layer = Dense(512, activation='relu')(layer)\n","  layer = Dense(512, activation='relu')(layer)\n","  layer = Dense(128, activation='relu')(layer)\n","  output_layer = Dense(1, activation='sigmoid')(layer)\n","\n","  model = models.Model(input_layer, output_layer)\n","\n","  start_time_train = time.time()\n","  model.compile(\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n","  model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=n_epochs, batch_size=512)\n","  end_time_train = time.time()\n","  time_train= (end_time_train - start_time_train)*1000\n","  time_train_val = round(time_train)\n","\n","\n","  accuracy = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=0)\n","  start_time_test = time.time()\n","  y_pred = model.predict(X_test)\n","  end_time_test = time.time()\n","  time_test= (end_time_test - start_time_test)*1000\n","  time_test_val=round(time_test)\n","  y_pred = np.around(y_pred, 0)\n","\n","  print(\"Accuracy: \", metrics.accuracy_score(y_pred, Y_test))\n","\n","  start_time_test = time.time()\n","  model.predict(X_test)\n","  end_time_test = time.time()\n","  time_test= (end_time_test - start_time_test)*1000\n","  time_test_val=round(time_test)\n","\n","  tracemalloc.start()\n","  model.predict(X_test)\n","  snapshot = tracemalloc.take_snapshot()\n","  top_stats = snapshot.statistics('traceback')\n","  stat = top_stats[0]\n","  mem_test = round(stat.size/1024)\n","  \n","  precision = metrics.precision_score(y_pred,Y_test)\n","  precision_val=round(precision,4)\n","\n","  recall= metrics.recall_score(y_pred,Y_test,average='macro')\n","  recal_val=round(recall,4)\n","\n","  f1=metrics.f1_score(y_pred,Y_test,average='macro')\n","  f1_val=round(f1,4)\n","\n","  accuracy_test = metrics.accuracy_score(y_pred,Y_test)\n","  accuracy_test_val = round(accuracy_test,4)\n","\n","  params = np.sum([np.prod(v.get_shape()) for v in model.trainable_weights])\n","\n","  export_dir=\"save_model/BRNN\"\n","\n","  model.save(export_dir)\n","\n","  model=tf.keras.models.load_model(export_dir)  \n","\n","  taille= os.stat('save_model/BRNN').st_size   \n","\n","  return [nom,n_epochs,drop_Out,precision_val,recal_val,f1_val,accuracy_test_val,time_train_val, time_test_val,mem_test,params,taille]"],"id":"68c3121b","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2623d9b3"},"source":["scores = []\n","def run_experiment():\n","  with open('save_model/resultat_BRNN.txt', 'w') as f:\n","    for dropOut in [0, 0.2,0.5,0.8]:\n","      score= evaluate_model_BRNN(nom='BRNN', X_data = X_data_tfidf_svd, Y_data = ylabels, n_epochs=20,drop_Out= dropOut)\n","      #print(score)\n","      f.write(\"{0}\".format(score))\n","      scores.append(score)\n","    return scores"],"id":"2623d9b3","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4f1488f8"},"source":["scores=run_experiment()"],"id":"4f1488f8","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"287bfc31"},"source":["## RCNN"],"id":"287bfc31"},{"cell_type":"code","metadata":{"id":"6535c179"},"source":["def evaluate_model_RCNN(nom, X_data,Y_data,n_epochs, drop_Out):\n","  verbose , batch_size = 0 , 32\n","  X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size=0.2, random_state=42,stratify=Y_data)\n","  \n","  input_layer = Input(shape=(300,))\n","\n","  layer = Reshape((10, 30))(input_layer)\n","  \n","  layer = Convolution1D(128, 7, activation=\"relu\")(layer)\n","  layer = Convolution1D(128, 3, activation=\"relu\")(layer)\n","  layer = Dropout(drop_Out)(layer)\n","  layer = Flatten()(layer)\n","  layer = Dense(512, activation='relu')(layer)\n","  layer = Dense(512, activation='relu')(layer)\n","  layer = Dense(128, activation='relu')(layer)\n","  output_layer = Dense(1, activation='sigmoid')(layer)\n","\n","\n","  model = models.Model(input_layer, output_layer)\n","\n","  start_time_train = time.time()\n","  model.compile(\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n","  model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=n_epochs, batch_size=512)\n","  end_time_train = time.time()\n","  time_train= (end_time_train - start_time_train)*1000\n","  time_train_val = round(time_train)\n","\n","\n","  accuracy = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=0)\n","  start_time_test = time.time()\n","  y_pred = model.predict(X_test)\n","  end_time_test = time.time()\n","  time_test= (end_time_test - start_time_test)*1000\n","  time_test_val=round(time_test)\n","  y_pred = np.around(y_pred, 0)\n","\n","  print(\"Accuracy: \", metrics.accuracy_score(y_pred, Y_test))\n","\n","  start_time_test = time.time()\n","  model.predict(X_test)\n","  end_time_test = time.time()\n","  time_test= (end_time_test - start_time_test)*1000\n","  time_test_val=round(time_test)\n","\n","  tracemalloc.start()\n","  model.predict(X_test)\n","  snapshot = tracemalloc.take_snapshot()\n","  top_stats = snapshot.statistics('traceback')\n","  stat = top_stats[0]\n","  mem_test = round(stat.size/1024)\n","  \n","  precision = metrics.precision_score(y_pred,Y_test,average='macro')\n","  precision_val=round(precision,4)\n","\n","  recall= metrics.recall_score(y_pred,Y_test,average='macro')\n","  recal_val=round(recall,4)\n","\n","  f1=metrics.f1_score(y_pred,Y_test,average='macro')\n","  f1_val=round(f1,4)\n","\n","  accuracy_test = metrics.accuracy_score(y_pred,Y_test)\n","  accuracy_test_val = round(accuracy_test,4)\n","\n","  params = np.sum([np.prod(v.get_shape()) for v in model.trainable_weights])\n","\n","  export_dir=\"save_model/RCNN\"\n","\n","  model.save(export_dir)\n","\n","  model=tf.keras.models.load_model(export_dir)  \n","\n","  taille= os.stat('save_model/RCNN').st_size   \n","\n","  return [nom,n_epochs,drop_Out,precision_val,recal_val,f1_val,accuracy_test_val,time_train_val, time_test_val,mem_test,params,taille]"],"id":"6535c179","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e4fe663a"},"source":["scores = []\n","def run_experiment():\n","  with open('save_model/resultat_RCNN.txt', 'w') as f:\n","    for dropOut in [0, 0.2,0.5,0.8]:\n","      score= evaluate_model_RCNN(nom='RCNN', X_data = X_data_tfidf_svd, Y_data = ylabels, n_epochs=20,drop_Out= dropOut)\n","      #print(score)\n","      f.write(\"{0}\".format(score))\n","      scores.append(score)\n","    return scores"],"id":"e4fe663a","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cffca75c"},"source":["scores=run_experiment()"],"id":"cffca75c","execution_count":null,"outputs":[]}]}