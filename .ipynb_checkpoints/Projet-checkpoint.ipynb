{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "anonymous-omega",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/romaindeprez/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/romaindeprez/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/romaindeprez/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "greenhouse-reason",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is Bernie Sanders really happening?</td>\n",
       "      <td>Watching Sanders surge to the front of the pac...</td>\n",
       "      <td>opinion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The London terror attack would’ve been much wo...</td>\n",
       "      <td>Two people were stabbed in Sunday’s attack in ...</td>\n",
       "      <td>opinion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Super Bowl poses the question: What’s more...</td>\n",
       "      <td>But one debate overshadowed the rest: whether ...</td>\n",
       "      <td>opinion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On health care, is Trump malicious or just inc...</td>\n",
       "      <td>While Democrats debate the best path to univer...</td>\n",
       "      <td>opinion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What ever happened to that ‘head on a pike’ st...</td>\n",
       "      <td>It was when Adam Schiff made a reference to so...</td>\n",
       "      <td>opinion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                Is Bernie Sanders really happening?   \n",
       "1  The London terror attack would’ve been much wo...   \n",
       "2  The Super Bowl poses the question: What’s more...   \n",
       "3  On health care, is Trump malicious or just inc...   \n",
       "4  What ever happened to that ‘head on a pike’ st...   \n",
       "\n",
       "                                                body    label  \n",
       "0  Watching Sanders surge to the front of the pac...  opinion  \n",
       "1  Two people were stabbed in Sunday’s attack in ...  opinion  \n",
       "2  But one debate overshadowed the rest: whether ...  opinion  \n",
       "3  While Democrats debate the best path to univer...  opinion  \n",
       "4  It was when Adam Schiff made a reference to so...  opinion  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"opinion_fact_news.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fabulous-harbor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "successful-music",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['opinion' 'fact']\n"
     ]
    }
   ],
   "source": [
    "print(df['label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-compatibility",
   "metadata": {},
   "source": [
    "Notre dataset contient 10 000 articles de faits et d'opinion.  \n",
    "Dont une colonne pour le titre de l'article, une pour son contenu et une dernière pour indiquer si il s'agit d'un fait ou d'une opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "broadband-consumer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "opinion    5000\n",
       "fact       5000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-furniture",
   "metadata": {},
   "source": [
    "Notre dataset est constitué de 5000 faits et 5000 opinions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-twins",
   "metadata": {},
   "source": [
    "## Prétraitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "prime-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretraitement(data):\n",
    "    stopword=\"\" #mot commun\n",
    "    cleantext = \" \" # texte que l'on aura prétraité\n",
    "    stopword = set(stopwords.words('english')) #mot commun anglais\n",
    "    \n",
    "    pattern1 = '[!.?$\\[\\]/\\}#=<>\"\\*:,|_~;()^\\']' #pattern pour les caractères spéciaux\n",
    "    pattern2 = '[\\n\\n]+' # pattern plusieurs lignes vides\n",
    "    pattern3 = '[\\ \\ ]+' # pattern plusieurs espaces\n",
    "    \n",
    "    for word in word_tokenize(data): #Pour chaque mot\n",
    "        if(any(car.isdigit() for car in word)): #S'il y a un caractère numérique dans le mot, on passe au suivant\n",
    "            continue\n",
    "        word=word.lower()\n",
    "        if (word not in stopword) and (len(word)>2): #Si le mot n'est pas dans nos stopword et est de longueur >= 3\n",
    "            word_pattern = re.sub(pattern1,'',word) #Suppression des caractères spéciaux\n",
    "            word_pattern = re.sub(pattern2,'\\n',word_pattern) #Suppression des lignes vides\n",
    "            word_pattern = re.sub(pattern3,' ',word_pattern) #Suppression des multi-espaces\n",
    "            cleaned_word = word_pattern.strip()\n",
    "            cleaned_word = lemmatizer.lemmatize(cleaned_word) # Lemmatisation\n",
    "            if(cleaned_word not in stopword) and (len(cleaned_word) > 2):\n",
    "                cleantext = cleantext + cleaned_word + \" \" #On ajoute le mot a notre texte prétraité\n",
    "    return cleantext.strip()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-saying",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|███████████████████▎                 | 5214/10000 [00:56<00:27, 176.96it/s]"
     ]
    }
   ],
   "source": [
    "df_pretraitement = df.copy()\n",
    "for ind_line in tqdm(range(len(df_pretraitement)), leave=True, position=0):\n",
    "    df_pretraitement.iloc[ind_line,0] = pretraitement(df_pretraitement.iloc[ind_line,0])\n",
    "    df_pretraitement.iloc[ind_line,1] = pretraitement(df_pretraitement.iloc[ind_line,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-bloom",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pretraitement.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-heather",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enregistrement dans un csv des données prétraitées\n",
    "df_pretraitement.to_csv(\"opinion_fact_news_pretraiter.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb3ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pretraitement.decribe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ba511f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
